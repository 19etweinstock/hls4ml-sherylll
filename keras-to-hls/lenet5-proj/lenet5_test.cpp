//
//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
//
//    Copyright (C) 2017 EJ Kreinar
//
//    This program is free software: you can redistribute it and/or modify
//    it under the terms of the GNU General Public License as published by
//    the Free Software Foundation, either version 3 of the License, or
//    (at your option) any later version.
//
//    This program is distributed in the hope that it will be useful,
//    but WITHOUT ANY WARRANTY; without even the implied warranty of
//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//    GNU General Public License for more details.
//
//    You should have received a copy of the GNU General Public License
//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
//
#include <fstream>
#include <iostream>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <string>
#include <cstring>
#include "firmware/parameters.h"
#include "firmware/lenet5.h"
#include "nnet_helpers.h"

#define IMAGE_WIDTH 28
#ifdef C_COSIM
  #define TEST_SIZE 10
#else
  #define TEST_SIZE 100 // full test set has 10000 samples
#endif


int max_likelihood(result_t y[N_OUTPUTS])
{
	int i_likely = 0;
	result_t y_max = 0;
	for (int i = 0; i < N_OUTPUTS; i++)
	{
		if (y[i] > y_max)
		{
			y_max = y[i];
			i_likely = i;
		}
	}
	return i_likely;
}

int read_to_array(char *path, input_t x_test[IMAGE_WIDTH*IMAGE_WIDTH*1], int *y_test)
{
	std::ifstream inFile;
	inFile.open(path);
	if (!inFile)
		return -1;
	if (inFile.get() == '#')
		inFile >> *y_test;
//	std::cout << *y_test;
	for (int i = 0; i < IMAGE_WIDTH; i++)
	{
		for (int j = 0; j < IMAGE_WIDTH; j++)
		{
			inFile >> x_test[i*IMAGE_WIDTH+j+0];
		}
	}
	inFile.close();
	return 0;
}

#define LENET 0
#define RELU 1
#define WEIGHT 2
#define BIAS_T 3
#define MULT 4
#define POOLING 5
#define CONVOLUTION 6
#define DENSE 7
#define DENSE_1Out 8
#define LENET_DEBUG_CONV 9
#define LENET_OUTPUT 10
#define LENET_DEBUG 11
#define POOL 12

#define TEST LENET

#if TEST == LENET_DEBUG || TEST == LENET_OUTPUT || TEST == LENET_DEBUG_CONV
#define im 0

#elif TEST == RELU
typedef ap_fixed<7,4> input_test_t;
typedef ap_fixed<2,1, AP_RND, AP_SAT> result_test_t;
typedef ap_ufixed<1,0, AP_RND_ZERO, AP_SAT> implicit_t;

#define NUM_TEST 48

struct relu_config_test : nnet::activ_config {
	static const unsigned n_in = NUM_TEST;
	static const unsigned table_size = 1024;
	static const unsigned io_type = nnet::io_parallel;
	};

#elif TEST == WEIGHT
#define NUM_TEST 6

typedef ap_fixed<2,0> weight_layer1_t;
typedef ap_fixed<2,-1> weight_layer2_t;

#elif TEST == BIAS_T
typedef ap_fixed<0,0> zero_t;
typedef ap_fixed<1,0> one_t;

#elif TEST == MULT
typedef ap_ufixed<1,0, AP_RND, AP_SAT> activation_t;
typedef ap_fixed<2,0> weight_layer1_t;
typedef ap_fixed<2,-1> weight_layer2_t;
typedef ap_fixed<7,3> result_test_t;

#define NUM_TEST 4

#elif TEST == POOLING
struct config_pooltest : nnet::pooling2d_config {
        static const unsigned in_height = 4;
        static const unsigned in_width = 4;
        static const unsigned n_filt = 2;
        static const unsigned stride_height = 2;
        static const unsigned stride_width = 2;
        static const unsigned pool_height = 2;
        static const unsigned pool_width = 2;
        static const unsigned out_height = 2;
        static const unsigned out_width = 2;
        static const unsigned pad_top = 0;
        static const unsigned pad_bottom = 0;
        static const unsigned pad_left = 0;
        static const unsigned pad_right = 0;
        static const nnet::Pool_Op pool_op = nnet::Max;
        static const unsigned reuse = 1;
    };

typedef ap_ufixed<1,0, AP_RND, AP_SAT> activation_t;

#elif TEST == CONVOLUTION

typedef ap_ufixed<1,0, AP_RND, AP_SAT> activation_t;
typedef ap_fixed<2,0> weight_layer1_t;
typedef ap_fixed<2,-1> weight_layer2_t;
typedef ap_fixed<5,2> result_layer1_t;
typedef ap_fixed<5,1> result_layer2_t;
typedef ap_fixed<1,0> one_t;

struct config_weight1 : nnet::conv2d_config {
        static const unsigned pad_top = 0;
        static const unsigned pad_bottom = 0;
        static const unsigned pad_left = 0;
        static const unsigned pad_right = 0;
        static const unsigned in_height = 4;
        static const unsigned in_width = 4;
        static const unsigned n_chan = 1;
        static const unsigned filt_height = 2;
        static const unsigned filt_width = 2;
        static const unsigned n_filt = 2;
        static const unsigned stride_height = 1;
        static const unsigned stride_width = 1;
        static const unsigned out_height = 3;
        static const unsigned out_width = 3;
        static const unsigned multiplier_limit = 86400;
        static const unsigned reuse_factor = 1;
        static const unsigned n_zeros = 0;
        static const bool store_weights_in_bram = false;
        typedef result_layer1_t accum_t;
        typedef one_t bias_t;
        typedef weight_layer1_t weight_t;
        };

struct config_weight2 : nnet::conv2d_config {
        static const unsigned pad_top = 0;
        static const unsigned pad_bottom = 0;
        static const unsigned pad_left = 0;
        static const unsigned pad_right = 0;
        static const unsigned in_height = 4;
        static const unsigned in_width = 4;
        static const unsigned n_chan = 1;
        static const unsigned filt_height = 2;
        static const unsigned filt_width = 2;
        static const unsigned n_filt = 2;
        static const unsigned stride_height = 1;
        static const unsigned stride_width = 1;
        static const unsigned out_height = 3;
        static const unsigned out_width = 3;
        static const unsigned multiplier_limit = 86400;
        static const unsigned reuse_factor = 1;
        static const unsigned n_zeros = 0;
        static const bool store_weights_in_bram = false;
        typedef result_layer2_t accum_t;
        typedef one_t bias_t;
        typedef weight_layer2_t weight_t;
        };

#elif TEST == DENSE

typedef ap_ufixed<1,0, AP_RND, AP_SAT> activation_t;
typedef ap_fixed<2,-1> weight_layer5_t;
typedef ap_fixed<7,3> result_layer5_t;
typedef ap_fixed<1,0> one_t;

struct config_testdense : nnet::layer_config {
        static const unsigned n_in = 6;
        static const unsigned n_out = 4;
        static const unsigned io_type = nnet::io_parallel;
        static const unsigned reuse_factor = 1;
        static const unsigned n_zeros = 0;
        static const bool store_weights_in_bram = false;
        typedef result_layer5_t accum_t;
        typedef one_t bias_t;
        typedef weight_layer5_t weight_t;
        };

#elif TEST == DENSE_1Out

typedef ap_ufixed<1,0, AP_RND, AP_SAT> activation_t;
typedef ap_fixed<2,-1> weight_layer5_t;
typedef ap_fixed<7,3> result_layer5_t;
typedef ap_fixed<1,0> one_t;

struct config_testdense : nnet::layer_config {
        static const unsigned n_in = 6;
        static const unsigned n_out = 1;
        static const unsigned io_type = nnet::io_parallel;
        static const unsigned reuse_factor = 1;
        static const unsigned n_zeros = 0;
        static const bool store_weights_in_bram = false;
        typedef result_layer5_t accum_t;
        typedef one_t bias_t;
        typedef weight_layer5_t weight_t;
        };

#elif TEST == POOL
typedef ap_fixed<6,6> type_t;

#endif

int main(int argc, char **argv)
{
	#if TEST == POOL

	type_t test[4][4][1] = {
		{{1},{2},{3},{4}},
		{{5},{6},{7},{8}},
		{{9},{10},{11},{12}},
		{{13},{14},{15},{16}}
	};

	type_t output[16] = {};

	nnet::flatten<type_t, 4, 4, 1>(test, output);

	for (int i = 0; i <4; i++){
		for (int j = 0; j <4; j++)
			std::cout << test[i][j][0] << " ";
		std::cout << std::endl;
	}
	std::cout << std::endl;

	for(int i=0; i <16; i++)
		std::cout << output[i] << " ";

	std::cout << std::endl;
	std::cout << std::endl;

	type_t test2[4][4][2] = {
		{{1,-1},{2,-2},{3,-3},{4,-4}},
		{{5,-5},{6,-6},{7,-7},{8,-8}},
		{{9,-9},{10,-10},{11,-11},{12,-12}},
		{{13,-13},{14,-14},{15,-15},{16,-16}}
	};

	type_t output2[32] = {};

	nnet::flatten<type_t, 4, 4, 2>(test2, output2);

	for (int i = 0; i <4; i++){
		for (int j = 0; j <4; j++)
			std::cout << test2[i][j][1] << " ";
		std::cout << std::endl;
	}
	std::cout << std::endl;

	for(int i=0; i <32; i++)
		std::cout << output2[i] << " ";

	std::cout << std::endl;

	#elif TEST == DENSE_1Out
	
	activation_t input[6]={0,0.5,0.5,0.5,0,0.5};
	result_layer5_t result[1]={0};

	weight_layer5_t weights[6]={0.125,0.125,0.125,0.1250,0.125,0.125};
	// weight_layer5_t weights[6]={-0.125,0.125,-0.125,0.125,-0.125,-0.125};
	// weight_layer5_t weights[6]={-0.125,-0.125,-0.125,0.1250,0.125,-0.125};
	// weight_layer5_t weights[6]={-0.125,0.125,0.125,-0.1250,-0.125,0.125};

	one_t bias[1]={0};

	nnet::compute_layer<activation_t,result_layer5_t,config_testdense>(input,result,weights,bias);

	std::cout << result[0] << std::endl;

	#elif TEST == DENSE

	activation_t input[6]={0,0.5,0.5,0.5,0,0.5};
	result_layer5_t result[4]={0};

	// weight_layer5_t weights_1d[24]={0.125,0.125,0.125,0.1250,0.125,0.125,
	// 								-0.125,0.125,-0.125,0.125,-0.125,-0.125,
	// 								-0.125,-0.125,-0.125,0.1250,0.125,-0.125,
	// 								-0.125,0.125,0.125,-0.1250,-0.125,0.125};

	weight_layer5_t weights_1d[24]={0.125,-0.125,-0.125,-0.125,
									0.125,0.125,-0.125,0.125,
									0.125,-0.125,-0.125,0.125,
									0.125,0.125,0.125,-0.125,
									0.125,-0.125,0.125,-0.125,
									0.125,-0.125,-0.125,0.125};

	one_t bias[4]={0,0,0,0};

	nnet::compute_layer<activation_t,result_layer5_t,config_testdense>(input,result,weights_1d,bias);

	for(int i = 0; i< 4; i++)
		std::cout << result[i] << std::endl;

	#elif TEST == CONVOLUTION

	activation_t input[4][4][1] = {
		{{0},{0},{0.5},{0}},
		{{0.5},{0.5},{0},{0.5},},
		{{0},{0.5},{0.5},{0.5}},
		{{0.5},{0},{0.5},{0.5}}
	};

	weight_layer1_t weights1[2][2][1][2]={
		{{{0.25,0.25}},{{0.25,0}}},
		{{{0.25,-0.25}},{{0.25,0.25}}}
	};

	weight_layer1_t weights1_1d[8]={0.25,0.25,0.25,0,0.25,-0.25,0.25,0.25};

	weight_layer2_t weights2[2][2][1][2]={
		{{{0.125,0.125}},{{0,-0.125}}},
		{{{0,-0.125}},{{-0.125,0}}}
	};

	weight_layer2_t weights2_1d[8]={0.125,0.125,0,-0.125,0,-0.125,-0.125,0};

	one_t bias[2] = {0,0};

	result_layer1_t result1[3][3][2] = {0};
	result_layer2_t result2[3][3][2] = {0};

	nnet::conv_2d<activation_t,result_layer1_t,config_weight1>(input,result1,weights1_1d,bias);
	nnet::conv_2d<activation_t,result_layer2_t,config_weight2>(input,result2,weights2_1d,bias);

	for(int i = 0; i < 3; i++){
		for (int j = 0; j <3; j++){
			std:: cout << result1[i][j][0] << " ";
		}
		std::cout << std::endl;
	}

	std::cout << std::endl;

	for(int i = 0; i < 3; i++){
		for (int j = 0; j <3; j++){
			std:: cout << result1[i][j][1] << " ";
		}
		std::cout << std::endl;
	}

	std::cout << std::endl;

	for(int i = 0; i < 3; i++){
		for (int j = 0; j <3; j++){
			std:: cout << result2[i][j][0] << " ";
		}
		std::cout << std::endl;
	}

	std::cout << std::endl;

	for(int i = 0; i < 3; i++){
		for (int j = 0; j <3; j++){
			std:: cout << result2[i][j][1] << " ";
		}
		std::cout << std::endl;
	}

	std::cout << std::endl;

	
	#elif TEST == POOLING

	activation_t input[4][4][2] = {
		{{0.5,0.5},{0,0.5},{0,0},{0,0}},
		{{0,0.5},{0,0.5},{0,0.5},{0,0.5}},
		{{0.5,0.5},{0,0.5},{0,0},{0,0}},
		{{0,0},{0.5,0},{0.5,0},{0,0.5}}
	};
	activation_t output[2][2][2] = {0};

    nnet::pooling2d<activation_t, config_pooltest>(input, output);

	for(int i = 0; i < 2; i++) {
		for (int j = 0; j < 2; j++)
			std::cout << output[i][j][0] << " ";
		std::cout << std::endl;
	}

	for(int i = 0; i < 2; i++) {
		for (int j = 0; j < 2; j++)
			std::cout << output[i][j][1] << " ";
		std::cout << std::endl;
	}



	#elif TEST == MULT
	weight_layer1_t layer1[NUM_TEST] = {-0.25, -0.25, 0.25, 0.25};
	weight_layer2_t layer2[NUM_TEST] = {-0.125, -0.125, 0.125, 0.125};
	activation_t  activate[NUM_TEST] = {0, 0.5, 0, 0.5};
	result_test_t result1[NUM_TEST] = {};
	result_test_t result2[NUM_TEST] = {};

	for (int i = 0; i < NUM_TEST; i++){
		result1[i] = layer1[i] * activate[i];
		result2[i] = layer2[i] * activate[i];
	}

	for(int i = 0; i < NUM_TEST; i++){
		std::cout << layer1[i] << " x " << activate[i] << " = " << result1[i] << std::endl;
	}
	for(int i = 0; i < NUM_TEST; i++){
		std::cout << layer2[i] << " x " << activate[i] << " = " << result2[i] << std::endl;
	}


	#elif TEST == BIAS_T
	// zero_t zero = 0;
	one_t one = 0;
	// std::cout << zero << std::endl;
	std::cout << one << std::endl;

	#elif TEST == WEIGHT

	weight_layer1_t layer1[NUM_TEST] = {-0.5, -0.25, -0.125, 0, 0.125, 0.25};
	weight_layer2_t layer2[NUM_TEST] = {-0.25, -0.125, -0.0625, 0, 0.0625, 0.125};

	for (int i = 0; i < NUM_TEST; i++)
		std::cout << layer1[i] << " " << layer2[i] << std::endl;


	#elif TEST == RELU
	input_test_t input[NUM_TEST]={1};
	for (int i = 0; i < NUM_TEST; i++)
		input[i]=i/16.0-1;

	result_test_t result[NUM_TEST]= {0};
	// implicit_t res_im[NUM_TEST] = {0};
	
	nnet::relu<input_test_t, result_test_t, relu_config_test>(input, result);
	for(int i = 0; i < NUM_TEST; i++){
		std::cout << input[i] << " " << result[i] << " " << (implicit_t) input[i] << std::endl;
	}


	#elif TEST == LENET


	input_t  data_str[IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1];
	input_t  input_str[IN_HEIGHT_1][IN_WIDTH_1][N_CHAN_1];

	result_t probs[N_OUTPUTS] = {0};
	int y_test, counter = 0;

	char x_str[10] = "";
	char path_cstr[30];

	for (int im=0; im < TEST_SIZE; im ++){
			sprintf(x_str, "%d.txt", im);
			std::string image_path = "../../../../test_images/";
			image_path += std::string(x_str);
			strcpy(path_cstr, image_path.c_str());
			if (read_to_array(path_cstr, data_str, &y_test) == 0){
				nnet::unflatten<input_t, IN_HEIGHT_1, IN_WIDTH_1, N_CHAN_1>(data_str, input_str);
				unsigned short size_in = IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1;
				unsigned short size_out =  N_OUTPUTS;
				lenet5(input_str, probs, size_in, size_out);

				int y_pred = max_likelihood(probs);
				std::cout << im << " " << (y_pred == y_test)<< " " << y_pred << " " << y_test << std::endl;
				if (y_pred == y_test)
					counter++;
			}
			else
				std::cout << "failed to read file" << std::endl;
	}
	std::cout << counter << std::endl;

	#elif TEST == LENET_DEBUG_CONV

	input_t  data_str[IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1];
	input_t  input_str[IN_HEIGHT_1][IN_WIDTH_1][N_CHAN_1];

	bias_default_t b1[6] = {0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000};

	weight_default_layer1_t w1[150] = {-0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000};


	result_t probs[N_OUTPUTS] = {0};
	int y_test, counter = 0;

	char x_str[10] = "";
	char path_cstr[30];

	sprintf(x_str, "%d.txt", im);
	std::string image_path = "../../../../test_images/";
	image_path += std::string(x_str);
	strcpy(path_cstr, image_path.c_str());
	if (read_to_array(path_cstr, data_str, &y_test) == 0){
		nnet::unflatten<input_t, IN_HEIGHT_1, IN_WIDTH_1, N_CHAN_1>(data_str, input_str);
		unsigned short size_in = IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1;
		unsigned short size_out =  N_OUTPUTS;

		// run the convolution
		accum_default_conv0_t layer1_out[OUT_HEIGHT_1][OUT_WIDTH_1][N_FILT_1];
		nnet::conv_2d<input_t, accum_default_conv0_t, config1>(input_str, layer1_out, w1, b1);

		for(int k = 0; k < N_FILT_1; k++){
			for(int i = 0; i < OUT_HEIGHT_1; i++){
				for(int j = 0; j < OUT_WIDTH_1; j++){
					std::cout << (layer1_out[i][j][k]) << " ";
				}
				std::cout << std::endl;
			}
			std::cout << std::endl;
		}

		input_t activated[OUT_HEIGHT_1][OUT_WIDTH_1][N_FILT_1];
		nnet::conv_2d<input_t, input_t, config1>(input_str, activated, w1, b1);

		for(int k = 0; k < N_FILT_1; k++){
			for(int i = 0; i < OUT_HEIGHT_1; i++){
				for(int j = 0; j < OUT_WIDTH_1; j++){
					std::cout << (activated[i][j][k] == 0.5) << " ";
				}
				std::cout << std::endl;
			}
			std::cout << std::endl;
		}
	}
	else
		std::cout << "failed to read file" << std::endl;

	#elif TEST == LENET_DEBUG

		input_t  data_str[IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1];
	input_t  data[IN_HEIGHT_1][IN_WIDTH_1][N_CHAN_1];

	bias_default_t b1[6] = {0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000};
	weight_default_layer1_t w1[150] = {-0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, 0.250000000000, -0.250000000000, 0.250000000000, -0.250000000000, -0.250000000000, -0.250000000000, 0.250000000000};
	bias_default_t b3[8] = {0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000, 0.000000000000};
	weight_default_t w3[1200] = {-0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, 0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000, 0.125000000000, -0.125000000000, -0.125000000000, -0.125000000000};


	result_t probs[N_OUTPUTS] = {0};
	int y_test, counter = 0;

	char x_str[10] = "";
	char path_cstr[30];

	sprintf(x_str, "%d.txt", im);
	std::string image_path = "../../../../test_images/";
	image_path += std::string(x_str);
	strcpy(path_cstr, image_path.c_str());
	if (read_to_array(path_cstr, data_str, &y_test) == 0){
		nnet::unflatten<input_t, IN_HEIGHT_1, IN_WIDTH_1, N_CHAN_1>(data_str, data);
		unsigned short size_in = IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1;
		unsigned short size_out =  N_OUTPUTS;

		// run the convolution
		input_t layer1_out[OUT_HEIGHT_1][OUT_WIDTH_1][N_FILT_1];
		nnet::conv_2d<input_t, input_t, config1>(data, layer1_out, w1, b1);

		input_t layer2_out[OUT_HEIGHT_2][OUT_WIDTH_2][N_FILT_2];
		nnet::pooling2d<input_t, config2>(layer1_out, layer2_out);

		for(int k = 0; k < 1; k++){
			for(int i = 0; i < OUT_HEIGHT_2; i++){
				for(int j = 0; j < OUT_WIDTH_2; j++){
					std::cout << (layer2_out[i][j][k] == 0.5) << " ";
				}
				std::cout << std::endl;
			}
			std::cout << std::endl;
		}

		input_t layer3_out[OUT_HEIGHT_3][OUT_WIDTH_3][N_FILT_3];
		accum_default_conv1_t layer3_temp[OUT_HEIGHT_3][OUT_WIDTH_3][N_FILT_3];
		nnet::conv_2d<input_t, accum_default_conv1_t, config3>(layer2_out, layer3_temp, w3, b3);
		nnet::conv_2d<input_t, input_t, config3>(layer2_out, layer3_out, w3, b3);

		for(int k = 0; k < 1; k++){
			for(int i = 0; i < OUT_HEIGHT_3; i++){
				for(int j = 0; j < OUT_WIDTH_3; j++){
					std::cout << (layer3_temp[i][j][k]) << " ";
				}
				std::cout << std::endl;
			}
			std::cout << std::endl;
		}
		
		for(int k = 0; k < 1; k++){
			for(int i = 0; i < OUT_HEIGHT_3; i++){
				for(int j = 0; j < OUT_WIDTH_3; j++){
					std::cout << (layer3_out[i][j][k]) << " ";
				}
				std::cout << std::endl;
			}
			std::cout << std::endl;
		}
		input_t layer4_out[OUT_HEIGHT_4*OUT_WIDTH_4*N_FILT_4];
		input_t pool2d_layer4_out[OUT_HEIGHT_4][OUT_WIDTH_4][N_FILT_4];
		nnet::pooling2d<input_t, config4>(layer3_out, pool2d_layer4_out);
		for(int k = 0; k < 1; k++){
			for(int i = 0; i < OUT_HEIGHT_4; i++){
				for(int j = 0; j < OUT_WIDTH_4; j++){
					std::cout << (pool2d_layer4_out[i][j][k]) << " ";
				}
				std::cout << std::endl;
			}
			std::cout << std::endl;
		}
		nnet::flatten<input_t, OUT_HEIGHT_4, OUT_WIDTH_4, N_FILT_4>(pool2d_layer4_out, layer4_out);

		for (int i = 0; i < 128; i++)
			std::cout << layer4_out[i] << " ";
	}
	else
		std::cout << "failed to read file" << std::endl;


	
	#elif TEST == LENET_OUTPUT

	input_t  data_str[IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1];
	input_t  input_str[IN_HEIGHT_1][IN_WIDTH_1][N_CHAN_1];

	result_t probs[N_OUTPUTS] = {0};
	int y_test, counter = 0;

	char x_str[10] = "";
	char path_cstr[30];

	sprintf(x_str, "%d.txt", im);
	std::string image_path = "../../../../test_images/";
	image_path += std::string(x_str);
	strcpy(path_cstr, image_path.c_str());
	if (read_to_array(path_cstr, data_str, &y_test) == 0){
		nnet::unflatten<input_t, IN_HEIGHT_1, IN_WIDTH_1, N_CHAN_1>(data_str, input_str);
		unsigned short size_in = IN_HEIGHT_1*IN_WIDTH_1*N_CHAN_1;
		unsigned short size_out =  N_OUTPUTS;
		lenet5(input_str, probs, size_in, size_out);
		for (int i =0; i < N_OUTPUTS; i++)
			std::cout << probs[i] << " ";
		std::cout << std::endl;
	}
	else
		std::cout << "failed to read file" << std::endl;

	#endif
	return 0;
}
